{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataGeneration import *\n",
    "import polars as pl\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import psutil\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset,DataLoader,random_split\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    print(f\"Memory usage: {process.memory_info().rss / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 7 sec\n"
     ]
    }
   ],
   "source": [
    "a = datagen()\n",
    "tic = time.perf_counter()\n",
    "a.apply_simulation()\n",
    "toc = time.perf_counter()\n",
    "print(f'Elapsed: {(toc - tic):.0f} sec')\n",
    "df = a.retrieve_df()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (171_000, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>C_A0</th><th>Q</th><th>T_initial</th><th>CA_initial</th><th>V(x)</th><th>test</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>list[list[f64]]</td></tr></thead><tbody><tr><td>-3.5</td><td>-500000.0</td><td>-71.387755</td><td>1.356122</td><td>339.778563</td><td>[[1.354603, 1.339532, … 1.225346], [-71.613715, -73.871187, … -91.71488]]</td></tr><tr><td>-3.5</td><td>-500000.0</td><td>-71.387755</td><td>1.478571</td><td>323.090516</td><td>[[1.476982, 1.461219, … 1.341897], [-71.613278, -73.866691, … -91.692486]]</td></tr><tr><td>-3.5</td><td>-500000.0</td><td>-71.387755</td><td>1.60102</td><td>338.189225</td><td>[[1.599361, 1.582902, … 1.458432], [-71.612825, -73.862031, … -91.66925]]</td></tr><tr><td>-3.5</td><td>-500000.0</td><td>-65.265306</td><td>1.111224</td><td>332.806468</td><td>[[1.109805, 1.095744, … 0.990109], [-65.493207, -67.770698, … -85.803362]]</td></tr><tr><td>-3.5</td><td>-500000.0</td><td>-65.265306</td><td>1.233673</td><td>285.531166</td><td>[[1.232181, 1.217404, … 1.106522], [-65.492642, -67.764885, … -85.774066]]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>3.5</td><td>500000.0</td><td>69.428571</td><td>-1.582653</td><td>326.868942</td><td>[[-1.579416, -1.54754, … -1.3348], [69.57569, 71.063144, … 84.521116]]</td></tr><tr><td>3.5</td><td>500000.0</td><td>69.428571</td><td>-1.460204</td><td>305.982645</td><td>[[-1.457284, -1.428704, … -1.250398], [69.588431, 71.209538, … 86.14405]]</td></tr><tr><td>3.5</td><td>500000.0</td><td>69.428571</td><td>-1.337755</td><td>316.883103</td><td>[[-1.335225, -1.31067, … -1.172527], [69.604813, 71.395907, … 88.092087]]</td></tr><tr><td>3.5</td><td>500000.0</td><td>69.428571</td><td>-1.215306</td><td>359.570317</td><td>[[-1.21324, -1.193443, … -1.101446], [69.624836, 71.622433, … 90.378135]]</td></tr><tr><td>3.5</td><td>500000.0</td><td>75.55102</td><td>-1.582653</td><td>362.089267</td><td>[[-1.579474, -1.548244, … -1.345292], [75.697988, 77.18704, … 90.893516]]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (171_000, 6)\n",
       "┌──────┬───────────┬────────────┬────────────┬────────────┬─────────────────────────────────┐\n",
       "│ C_A0 ┆ Q         ┆ T_initial  ┆ CA_initial ┆ V(x)       ┆ test                            │\n",
       "│ ---  ┆ ---       ┆ ---        ┆ ---        ┆ ---        ┆ ---                             │\n",
       "│ f64  ┆ f64       ┆ f64        ┆ f64        ┆ f64        ┆ list[list[f64]]                 │\n",
       "╞══════╪═══════════╪════════════╪════════════╪════════════╪═════════════════════════════════╡\n",
       "│ -3.5 ┆ -500000.0 ┆ -71.387755 ┆ 1.356122   ┆ 339.778563 ┆ [[1.354603, 1.339532, … 1.2253… │\n",
       "│ -3.5 ┆ -500000.0 ┆ -71.387755 ┆ 1.478571   ┆ 323.090516 ┆ [[1.476982, 1.461219, … 1.3418… │\n",
       "│ -3.5 ┆ -500000.0 ┆ -71.387755 ┆ 1.60102    ┆ 338.189225 ┆ [[1.599361, 1.582902, … 1.4584… │\n",
       "│ -3.5 ┆ -500000.0 ┆ -65.265306 ┆ 1.111224   ┆ 332.806468 ┆ [[1.109805, 1.095744, … 0.9901… │\n",
       "│ -3.5 ┆ -500000.0 ┆ -65.265306 ┆ 1.233673   ┆ 285.531166 ┆ [[1.232181, 1.217404, … 1.1065… │\n",
       "│ …    ┆ …         ┆ …          ┆ …          ┆ …          ┆ …                               │\n",
       "│ 3.5  ┆ 500000.0  ┆ 69.428571  ┆ -1.582653  ┆ 326.868942 ┆ [[-1.579416, -1.54754, … -1.33… │\n",
       "│ 3.5  ┆ 500000.0  ┆ 69.428571  ┆ -1.460204  ┆ 305.982645 ┆ [[-1.457284, -1.428704, … -1.2… │\n",
       "│ 3.5  ┆ 500000.0  ┆ 69.428571  ┆ -1.337755  ┆ 316.883103 ┆ [[-1.335225, -1.31067, … -1.17… │\n",
       "│ 3.5  ┆ 500000.0  ┆ 69.428571  ┆ -1.215306  ┆ 359.570317 ┆ [[-1.21324, -1.193443, … -1.10… │\n",
       "│ 3.5  ┆ 500000.0  ┆ 75.55102   ┆ -1.582653  ┆ 362.089267 ┆ [[-1.579474, -1.548244, … -1.3… │\n",
       "└──────┴───────────┴────────────┴────────────┴────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.df_to_tensor()\n",
    "a.standard_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_len, hidden_size_1, hidden_size_2, out_len):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.LSTM(input_len, hidden_size_1, batch_first=True),\n",
    "            nn.LSTM(hidden_size_1, hidden_size_2, batch_first=True),\n",
    "            nn.Linear(hidden_size_2, out_len)\n",
    "        )\n",
    "        \n",
    "        self._reinitialize()\n",
    "                \n",
    "    def _reinitialize(self):\n",
    "            \"\"\"\n",
    "            Tensorflow/Keras-like initialization\n",
    "            \"\"\"\n",
    "\n",
    "            \n",
    "\n",
    "            for name, p in self.named_parameters():\n",
    "                if 'lstm' in name:\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(p.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(p.data)\n",
    "                    elif 'bias_ih' in name:\n",
    "                        p.data.fill_(0)\n",
    "                        # Set forget-gate bias to 1\n",
    "                        n = p.size(0)\n",
    "                        p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                    elif 'bias_hh' in name:\n",
    "                        p.data.fill_(0)\n",
    "                elif 'fc' in name:\n",
    "                    if 'weight' in name:\n",
    "                        nn.init.xavier_uniform_(p.data)\n",
    "                    elif 'bias' in name:\n",
    "                        p.data.fill_(0)\n",
    "                        \n",
    "            \"\"\"code stolen from https://www.kaggle.com/code/junkoda/pytorch-lstm-with-tensorflow-like-initialization\"\"\"            \n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        h1 = torch.zeros(1, x.size(0), self.model[0].hidden_size).to(x.device)\n",
    "        c1 = torch.zeros(1, x.size(0), self.model[0].hidden_size).to(x.device)\n",
    "        h2 = torch.zeros(1, x.size(0), self.model[1].hidden_size).to(x.device)\n",
    "        c2 = torch.zeros(1, x.size(0), self.model[1].hidden_size).to(x.device)\n",
    "        \n",
    "\n",
    "        x, (h1, c1) = self.model[0](x, (h1, c1))\n",
    "        \n",
    "\n",
    "        x, (h2, c2) = self.model[1](x, (h2, c2))\n",
    "        \n",
    "  \n",
    "        x = self.model[2](x)  \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, input_len, hidden_size_1, hidden_size_2, out_len):\n",
    "#         super(RNN, self).__init__()\n",
    "        \n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.GRU(input_len, hidden_size_1, batch_first=True),\n",
    "#             nn.GRU(hidden_size_1, hidden_size_2, batch_first=True),\n",
    "#             nn.Linear(hidden_size_2, out_len)\n",
    "#         )\n",
    "\n",
    "#     self._reinitialize()\n",
    "                \n",
    "#     def _reinitialize(self):\n",
    "#             \"\"\"\n",
    "#             Tensorflow/Keras-like initialization\n",
    "#             \"\"\"\n",
    "#             for name, p in self.named_parameters():\n",
    "#                 if 'lstm' in name:\n",
    "#                     if 'weight_ih' in name:\n",
    "#                         nn.init.xavier_uniform_(p.data)\n",
    "#                     elif 'weight_hh' in name:\n",
    "#                         nn.init.orthogonal_(p.data)\n",
    "#                     elif 'bias_ih' in name:\n",
    "#                         p.data.fill_(0)\n",
    "#                         # Set forget-gate bias to 1\n",
    "#                         n = p.size(0)\n",
    "#                         p.data[(n // 4):(n // 2)].fill_(1)\n",
    "#                     elif 'bias_hh' in name:\n",
    "#                         p.data.fill_(0)\n",
    "#                 elif 'fc' in name:\n",
    "#                     if 'weight' in name:\n",
    "#                         nn.init.xavier_uniform_(p.data)\n",
    "#                     elif 'bias' in name:\n",
    "#                         p.data.fill_(0)\n",
    "#           \"\"\"code stolen from https://www.kaggle.com/code/junkoda/pytorch-lstm-with-tensorflow-like-initialization\"\"\" \n",
    "    \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         h1 = torch.zeros(1, x.size(0), self.model[0].hidden_size).to(x.device)\n",
    "      \n",
    "#         h2 = torch.zeros(1, x.size(0), self.model[1].hidden_size).to(x.device)\n",
    "       \n",
    "        \n",
    "\n",
    "#         x, _ = self.model[0](x, h1)\n",
    "        \n",
    "\n",
    "#         x, _= self.model[1](x, h2)\n",
    "        \n",
    "  \n",
    "#         x = self.model[2](x)  \n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(mp.cpu_count())\n",
    "device = torch.device('mps')\n",
    "\n",
    "Model = RNN(4,64,32,2)\n",
    "Model = Model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(Model.parameters(),lr=0.001,eps=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name , param in Model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs=100, batch_size=256,val_split=0.1):\n",
    "    \n",
    "    val_split = val_split\n",
    "    val_size = int(val_split * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    batch_size = 256\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=mp.cpu_count(),\n",
    "                              pin_memory=True,persistent_workers = True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,num_workers=mp.cpu_count(),\n",
    "                            pin_memory=True,persistent_workers = True)\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    train_list=[]\n",
    "    val_list= []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val = X_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                output = model(X_val)\n",
    "                loss = criterion(output, y_val)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(val_loader)\n",
    "        # if epoch+1 % 10 == 0:\n",
    "        toc = time.perf_counter()\n",
    "        print(f'Epoch: {epoch}, Elapsed: {(toc - tic):.0f} sec')\n",
    "        tic = time.perf_counter()\n",
    "        print(\"Loss for Training on Epoch \" + str(epoch) + \" is \" + str(val_loss))\n",
    "        val_list.append(val_loss)\n",
    "        train_list.append(train_loss)\n",
    "    return train_list,val_list\n",
    "            #print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_np= a.X_train_norm.astype(np.float32)\n",
    "y_train_np= a.y_train_norm.astype(np.float32)\n",
    "X_train_tensor = torch.from_numpy(X_train_np)\n",
    "y_train_tensor = torch.from_numpy(y_train_np)\n",
    "# X_train_tensor = torch.FloatTensor(a.X_train.astype(np.float32))\n",
    "# y_train_tensor = torch.FloatTensor(a.y_train.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloss,valloss=train(Model, dataset, epochs=100, batch_size=256,val_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Model.eval()\n",
    "    y_pred = Model(torch.from_numpy(a.X_test_norm.astype(np.float32)).to(device))\n",
    "    test_loss = criterion(y_pred, torch.from_numpy(a.y_test_norm.astype(np.float32)).to(device))\n",
    "    print('test loss is {}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(trainloss) + 1)\n",
    "plt.plot(epochs, trainloss, 'r', label='Training loss')\n",
    "plt.plot(epochs, valloss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "class tensorflow_LSTM:\n",
    "    def __init__(self,input_shape,hidden_unit1,hidden_unit2,num_output_variable):\n",
    "\n",
    "      self.Input_layer = Input(shape=(None,input_shape), dtype='float32') \n",
    "      x = LSTM(units = hidden_unit1, return_sequences= True)(self.Input_layer)\n",
    "      x = LSTM(units = hidden_unit2, return_sequences= True)(x)\n",
    "      x = Dense(units = num_output_variable,activation='linear')(x)\n",
    "\n",
    "      self.model = Model(inputs= self.Input_layer,outputs=x)\n",
    "\n",
    "      \n",
    "\n",
    "    def get_model(self):\n",
    "         return self.model\n",
    "\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Dense, Input,GRU\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# class tensorflow_LSTM:\n",
    "#     def __init__(self,input_shape,hidden_unit1,hidden_unit2,num_output_variable):\n",
    "\n",
    "#       self.Input_layer = Input(shape=(None,input_shape), dtype='float32') \n",
    "#       x = GRU(units = hidden_unit1, return_sequences= True)(self.Input_layer)\n",
    "#       x = GRU(units = hidden_unit2, return_sequences= True)(x)\n",
    "#       x = Dense(units = num_output_variable,activation='linear')(x)\n",
    "\n",
    "#       self.model = Model(inputs= self.Input_layer,outputs=x)\n",
    "\n",
    "      \n",
    "\n",
    "#     def get_model(self):\n",
    "#          return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tensorflow_LSTM(4,64,32,2)\n",
    "model=model.get_model()\n",
    "model.compile(\n",
    "      loss=MeanSquaredError(),\n",
    "      optimizer=Adam())\n",
    "\n",
    "history = model.fit(a.X_train_norm, a.y_train_norm, epochs=100, batch_size=256, validation_split=0.1, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(a.X_test_norm,a.y_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tensorflow\"\"\"\n",
    "\n",
    "import joblib\n",
    "x_scaler = joblib.load('standardscaler/xscaler.pkl')\n",
    "y_scaler = joblib.load('standardscaler/yscaler.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = np.linspace(-100, 100, 100000, endpoint=True)\n",
    "\n",
    "x_upper = list()\n",
    "x_lower = list()\n",
    "y_plot = list()\n",
    "\n",
    "for i in y:\n",
    "    sqrt = np.sqrt(-2688000 * i**2 + 15772800000)\n",
    "    if sqrt >= 0:\n",
    "        y_plot.append(i)\n",
    "        x_upper.append((-4400 * i + sqrt) / 212000)\n",
    "        x_lower.append((-4400 * i - sqrt) / 212000)\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "X_test = a.X_test_norm.astype(np.float32)\n",
    "y_test = a.y_test\n",
    "\n",
    "# plot the first 10 samples and their trajectories\n",
    "y_predict = model.predict(a.X_test_norm)\n",
    "y_predict = y_scaler.inverse_transform(y_predict.reshape(-1,2)).reshape(-1,10,2)\n",
    "X_plot =  a.X_test\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    if i == 0:  # only add label to 1 data point\n",
    "        plt.plot(X_plot[i, 0, 0], X_plot[i, 0, 1], marker=\"*\", markersize=15,color='orange')\n",
    "        plt.plot(y_test[i, :, 0], y_test[i, :, 1], color='cyan', lw=2, label='Test')\n",
    "        plt.plot(y_predict[i, :, 0], y_predict[i, :, 1], color='black', lw=2, ls=':', label='Predicted')\n",
    "    else:\n",
    "        plt.plot(X_plot[i, 0, 0], X_plot[i, 0, 1], marker=\"*\", markersize=15,color='orange')\n",
    "        plt.plot(y_test[i, :, 0], y_test[i, :, 1], color='cyan', lw=2)\n",
    "        plt.plot(y_predict[i, :, 0], y_predict[i, :, 1], color='black', lw=2, ls=':')\n",
    "\n",
    "    \n",
    "# plot stability region        \n",
    "plt.plot(x_lower, y_plot, color='steelblue')\n",
    "plt.plot(x_upper, y_plot, color='steelblue')\n",
    "plt.ylim([-100, 100])\n",
    "plt.xlim([-2, 2])\n",
    "\n",
    "plt.xlabel(\"C_A - C_As\")\n",
    "plt.ylabel(\"T - T_s\")\n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pytorch\"\"\"\n",
    "\n",
    "import joblib\n",
    "x_scaler = joblib.load('standardscaler/xscaler.pkl')\n",
    "y_scaler = joblib.load('standardscaler/yscaler.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = np.linspace(-100, 100, 100000, endpoint=True)\n",
    "\n",
    "x_upper = list()\n",
    "x_lower = list()\n",
    "y_plot = list()\n",
    "\n",
    "for i in y:\n",
    "    sqrt = np.sqrt(-2688000 * i**2 + 15772800000)\n",
    "    if sqrt >= 0:\n",
    "        y_plot.append(i)\n",
    "        x_upper.append((-4400 * i + sqrt) / 212000)\n",
    "        x_lower.append((-4400 * i - sqrt) / 212000)\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "X_test = torch.from_numpy(a.X_test_norm.astype(np.float32))\n",
    "y_test = a.y_test\n",
    "X_test = X_test.to(device)\n",
    "\n",
    "# plot the first 10 samples and their trajectories\n",
    "Model.eval()\n",
    "y_predict = Model(X_test).cpu()\n",
    "y_predict = y_scaler.inverse_transform(y_predict.detach().numpy().reshape(-1,2)).reshape(-1,10,2)\n",
    "#y_test =  y_scaler.inverse_transform(y_test.numpy().reshape(-1,2)).reshape(-1,10,2)\n",
    "X_plot =  a.X_test\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    if i == 0:  # only add label to 1 data point\n",
    "        plt.plot(X_plot[i, 0, 0], X_plot[i, 0, 1], marker=\"*\", markersize=15,color='orange')\n",
    "        plt.plot(y_test[i, :, 0], y_test[i, :, 1], color='cyan', lw=2, label='Test')\n",
    "        plt.plot(y_predict[i, :, 0], y_predict[i, :, 1], color='black', lw=2, ls=':', label='Predicted')\n",
    "    else:\n",
    "        plt.plot(X_plot[i, 0, 0], X_plot[i, 0, 1], marker=\"*\", markersize=15,color='orange')\n",
    "        plt.plot(y_test[i, :, 0], y_test[i, :, 1], color='cyan', lw=2)\n",
    "        plt.plot(y_predict[i, :, 0], y_predict[i, :, 1], color='black', lw=2, ls=':')\n",
    "\n",
    "    \n",
    "# plot stability region        \n",
    "plt.plot(x_lower, y_plot, color='steelblue')\n",
    "plt.plot(x_upper, y_plot, color='steelblue')\n",
    "plt.ylim([-100, 100])\n",
    "plt.xlim([-2, 2])\n",
    "\n",
    "plt.xlabel(\"C_A - C_As\")\n",
    "plt.ylabel(\"T - T_s\")\n",
    "plt.legend()\n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
